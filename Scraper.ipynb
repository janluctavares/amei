{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from unidecode import unidecode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função que extrai as info para cada anuncio\n",
    "def get_content(propiedade, data = {}):\n",
    "\n",
    "    aux = []\n",
    "    content = propiedade.find(\"div\",class_=\"content\")\n",
    "    classe = 'headContent'\n",
    "\n",
    "    aux = content.find(\"div\",class_=classe)\n",
    "    aux = aux.find(\"h4\").text.strip()\n",
    "\n",
    "    if \"comprar\" in aux:\n",
    "        aux = aux.replace(\"para comprar\",\"\").strip()\n",
    "        data[\"TITULO\"].append(aux)\n",
    "        data[\"OFERTA\"].append(\"COMPRA\")\n",
    "    elif \"alugar\" in aux:\n",
    "        aux = aux.replace(\"para alugar\",\"\").strip()\n",
    "        data[\"TITULO\"].append(aux)\n",
    "        data[\"OFERTA\"].append(\"ALUGAR\")\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            aux = propiedade.find(\"div\",class_=\"RegramesDeDesconto\")\n",
    "            aux = aux.find(\"h1\").text.replace(\"R$\",\"\").replace(\".\",\"\").replace(\"*\",\"\").strip().split(\"por\")\n",
    "            valor = aux[0].split(\" \")[-1].strip()\n",
    "            if valor.isnumeric():\n",
    "                data[\"VALORES\"].append(valor)\n",
    "            else:\n",
    "                aux2 = propiedade.find(\"div\",class_=\"RegramesDeDesconto\").find(\"p\").find(\"span\").text.strip()\n",
    "                data[\"VALORES\"].append(aux2.replace(\"R$\",\"\").replace(\".\",\"\").split(\" \")[-1].strip())\n",
    "            data[\"VALORES_PROMO\"].append(aux[1].strip())\n",
    "        except:\n",
    "            aux = propiedade.find(\"div\", class_=\"RegraMesesDesconto\")\n",
    "            aux = aux.find(\"h1\").text.replace(\"R$\",\"\").replace(\".\",\"\").replace(\"*\",\"\").strip().split(\"por\")\n",
    "            data[\"VALORES\"].append(aux[0].split(\" \")[-1].strip())\n",
    "            data[\"VALORES_PROMO\"].append(aux[1].strip())\n",
    "    except:\n",
    "        aux = propiedade.find(\"div\",class_=\"RegraSemDesconto\")\n",
    "        price = aux.find(\"span\").text.replace(\".\",\"\").replace(\"R$\",\"\").strip()\n",
    "        if not price.isnumeric():\n",
    "            # CORREÇÃO DO ERRO PERCEBIDO NA BASE DE DADOS, ALGUNS IMOVEIS\n",
    "            # APRESENTAM COMPOSIÇÃO DIFERENTE DE \"SPANS\"\n",
    "            # NECESSÁRIO SELECIONAR O SEGUNDO <SPAN>\n",
    "            price = aux.find_all(\"span\")[1].text.replace(\".\",\"\").replace(\"R$\",\"\").strip()\n",
    "        data[\"VALORES\"].append(price)\n",
    "        data[\"VALORES_PROMO\"].append(price)\n",
    "    try:\n",
    "        street = content.find(\"div\", class_ = \"RuaContainer\")\n",
    "        street = street.find_all(\"span\")\n",
    "        street = [st.text.strip() for st in street]\n",
    "        data[\"RUA/AV\"].append(street[0])\n",
    "        data[\"LOGRADOURO\"].append(street[1])\n",
    "    except:\n",
    "        data[\"RUA/AV\"].append(\"UNKNOWN\")\n",
    "        data[\"LOGRADOURO\"].append(\"UNKNOWN\")\n",
    "    try:\n",
    "        aux = content.find(\"div\",class_= \"Location\")\n",
    "        aux = aux.find(\"span\").text.split(\",\")\n",
    "        data[\"LOCATION\"].append(aux[0].strip())\n",
    "        aux = aux[1].split(\"-\")\n",
    "        data[\"CITY\"].append(aux[0].strip())\n",
    "        data[\"STATE\"].append(aux[1].strip())\n",
    "    except:\n",
    "        data[\"LOCATION\"].append(\"Unknown\")\n",
    "        data[\"CITY\"].append(\"Porto Alegre\")\n",
    "        data[\"STATE\"].append(\"RS\")\n",
    "    \n",
    "    classe = \"Details\"\n",
    "    aux = content.find(\"div\",class_=classe)\n",
    "    aux = aux.find_all(\"div\")\n",
    "\n",
    "    try:\n",
    "        data[\"METRAGEM\"].append(aux[0].text.replace(\"m²\",\"\").strip())\n",
    "    except:\n",
    "        data[\"METRAGEM\"].append(0)\n",
    "\n",
    "    try:\n",
    "        tgt = None\n",
    "        for img in range(len(aux)):\n",
    "            if aux[img].find('img', alt = True)['alt'] == \"Quartos\":\n",
    "                tgt = img\n",
    "        data[\"QUARTO\"].append(aux[tgt].text)\n",
    "    except:\n",
    "        data[\"QUARTO\"].append(0)\n",
    "\n",
    "    try:\n",
    "        tgt = None\n",
    "        for img in range(len(aux)):\n",
    "            if aux[img].find('img', alt = True)['alt'] == \"Garagens\":\n",
    "                tgt = img\n",
    "        data[\"GARAGEM\"].append(aux[tgt].text)\n",
    "    except:\n",
    "        data[\"GARAGEM\"].append(0)\n",
    "\n",
    "    try:\n",
    "        tgt = None\n",
    "        for img in range(len(aux)):\n",
    "            if aux[img].find('img', alt = True)['alt'] == \"Banheiros\":\n",
    "                tgt = img\n",
    "        data[\"BANHEIRO\"].append(aux[tgt].text)\n",
    "    except:\n",
    "        data[\"BANHEIRO\"].append(0)\n",
    "\n",
    "    try:\n",
    "        tag = []\n",
    "        classe = \"tags\"\n",
    "        aux = content.find(\"div\", class_ = classe)\n",
    "        aux = aux.find_all(\"h1\")\n",
    "        for tg in aux:\n",
    "            tag.append(tg.text)\n",
    "        data[\"TAG\"].append(\"|\".join(tag))\n",
    "    except:\n",
    "        data[\"TAG\"].append([])\n",
    "\n",
    "    classe = \"footer\"\n",
    "    aux = content.find(\"div\",class_ = classe)\n",
    "    aux = aux.find_all(\"div\")\n",
    "    data[\"KEY\"].append(aux[1].text.replace(\"ref: \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_data(\n",
    "        date = \"28-07-2023\", op = \"comprar\", category = \"residencial\", file_name = \"DatasetAux\", i = 1\n",
    "    ):\n",
    "    file_name = date.replace(\"-\",\"_\") + \"_\" + file_name + \"_\" + category.capitalize()\n",
    "    # URL PADRÃO DA AUXILIADORA PREDIAL\n",
    "    url = 'https://www.auxiliadorapredial.com.br/' + op + '/' + category + '/rs+porto-alegre?page={}'\n",
    "    # VARIAVEIS DE CONTROLE\n",
    "    extraction, head = True, True\n",
    "    if os.path.isfile(file_name + \".txt\"):\n",
    "        head = False\n",
    "    # LOOP INDEFINIDO\n",
    "    while extraction:\n",
    "        # DICIONARIO DE VARIAVEIS DE INTERESSE\n",
    "        data = {\n",
    "            \"TITULO\": [],\n",
    "            \"OFERTA\": [],\n",
    "            \"VALORES\": [],\n",
    "            \"VALORES_PROMO\": [],\n",
    "            \"RUA/AV\": [],\n",
    "            \"LOGRADOURO\": [],\n",
    "            \"LOCATION\": [],\n",
    "            \"CITY\": [],\n",
    "            \"STATE\": [],\n",
    "            \"METRAGEM\": [],\n",
    "            \"QUARTO\": [],\n",
    "            \"GARAGEM\": [],\n",
    "            \"BANHEIRO\": [],\n",
    "            \"TAG\": [],\n",
    "            \"KEY\": []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # SCRAP DAS INFO DO SITE\n",
    "            pagina = url.format(i)\n",
    "            site = requests.get(pagina)\n",
    "            soup = BeautifulSoup(site.text, \"html.parser\")\n",
    "            property_listings = soup.find_all(\"div\", class_=\"sc-c86ab3c9-0 fRIVEM\")\n",
    "\n",
    "            for property in property_listings:\n",
    "                # INFORMAÇÕES DE UMA PÁGINA EM SI\n",
    "                get_content(property, data = data)\n",
    "\n",
    "            # PREPARA O DATAFRAME\n",
    "            dataset = pd.DataFrame.from_dict(data, orient=\"index\").transpose()\n",
    "            #dataset[\"REF_DATE\"] = date\n",
    "            # MANDA PRA ARQUIVO AUXILIAR \n",
    "            dataset.to_csv(file_name + \".txt\", index = False, header = head, mode = \"a\")\n",
    "            print(\"Dados extraídos, página: \", i)\n",
    "\n",
    "            # ATUALIZA A RODAGEM\n",
    "            i += 1\n",
    "            head = False\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            # CONDIÇÃO DE TÉRMINO\n",
    "            if dataset.empty:\n",
    "                extraction = False\n",
    "\n",
    "        except Exception as exc:\n",
    "            # ROLOU ALGUMA MERDA\n",
    "            extraction = False\n",
    "            print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='www.auxiliadorapredial.com.br', port=443): Max retries exceeded with url: /comprar/residencial/rs+porto-alegre?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F5EBB8DE50>: Failed to resolve 'www.auxiliadorapredial.com.br' ([Errno 11001] getaddrinfo failed)\"))\n",
      "HTTPSConnectionPool(host='www.auxiliadorapredial.com.br', port=443): Max retries exceeded with url: /alugar/residencial/rs+porto-alegre?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F5EBBF0CD0>: Failed to resolve 'www.auxiliadorapredial.com.br' ([Errno 11001] getaddrinfo failed)\"))\n",
      "HTTPSConnectionPool(host='www.auxiliadorapredial.com.br', port=443): Max retries exceeded with url: /comprar/comercial/rs+porto-alegre?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F5EBB24D30>: Failed to resolve 'www.auxiliadorapredial.com.br' ([Errno 11001] getaddrinfo failed)\"))\n",
      "HTTPSConnectionPool(host='www.auxiliadorapredial.com.br', port=443): Max retries exceeded with url: /alugar/comercial/rs+porto-alegre?page=1 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F5EBBEFD60>: Failed to resolve 'www.auxiliadorapredial.com.br' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "date = \"2023-08-26\"\n",
    "# EXECUÇÃO DO ALGORITMO DE SCRAPING\n",
    "get_data(date = date, op = \"comprar\", file_name = \"Dataset_AP_Comprar\", category = \"residencial\")\n",
    "get_data(date = date, op = \"alugar\", file_name = \"Dataset_AP_Alugar\", category = \"residencial\")\n",
    "\n",
    "get_data(date = date, op = \"comprar\", file_name = \"Dataset_AP_Comprar\", category = \"comercial\")\n",
    "get_data(date = date, op = \"alugar\", file_name = \"Dataset_AP_Alugar\", category = \"comercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_in_shape(date, file_name):\n",
    "\n",
    "    def get_unique_tags(tgs):\n",
    "        aux, ntags = [], []\n",
    "        for c in tgs.columns:\n",
    "            aux += tgs[c].unique().tolist()\n",
    "        aux = list(set(aux))\n",
    "        for t in aux:\n",
    "            if type(t) == str: ntags.append(t)\n",
    "        return(sorted(ntags))\n",
    "    \n",
    "    def cols_buy(cls):\n",
    "        aux = []\n",
    "        for c in cls:\n",
    "            c2 = unidecode(c).upper().replace(\" \",\"_\")\n",
    "            if c2 == \"METRAGEM\": c2 = \"AREA\"\n",
    "            if c2 == \"TITULO\": c2 = \"TIPO\"\n",
    "            aux.append(c2)\n",
    "        return(aux)\n",
    "\n",
    "    df = pd.read_csv(date.replace(\"-\",\"_\") + \"_\" + file_name+ \".txt\")\n",
    "    # TRATAMENTO DAS TAGS\n",
    "    tags = df[\"TAG\"].str.split(\"|\", expand=True)\n",
    "    new_tags = pd.DataFrame(0, index = tags.index, columns = get_unique_tags(tgs = tags))\n",
    "    for i in tags.index:\n",
    "        features = tags.loc[i,:].dropna().tolist()\n",
    "        for feat in features:\n",
    "            new_tags.loc[i,feat] = 1\n",
    "    df.drop(labels = [\"TAG\"], axis = 1, inplace = True)\n",
    "    df = pd.concat([df, new_tags], axis = 1)\n",
    "    if \"compra\" in file_name.lower() or \"aluga\" in file_name.lower():\n",
    "        # TRATAMENTO COMRPA\n",
    "        df[\"VALORES\"] = df[\"VALORES\"].astype(int)\n",
    "        df[\"VALORES_PROMO\"] = df[\"VALORES_PROMO\"].astype(int)\n",
    "        df[\"VALORES\"][df.VALORES < 100] = np.nan\n",
    "        df[\"VALORES_PROMO\"][df.VALORES_PROMO < 100] = np.nan\n",
    "        df[\"METRAGEM\"][df.METRAGEM <= 0] = np.nan\n",
    "        df[\"LOCATION\"] = df[\"LOCATION\"].apply(lambda x: unidecode(x).strip().upper())\n",
    "        df[\"TITULO\"] = df[\"TITULO\"].apply(lambda x: unidecode(x).strip().upper())\n",
    "        df[\"CITY\"] = df[\"CITY\"].apply(lambda x: unidecode(x).strip().upper())\n",
    "        df[\"LOGRADOURO\"] = df[\"LOGRADOURO\"].apply(lambda x: unidecode(x).strip().upper())\n",
    "        # TRATAMENTO RUAS\n",
    "        df[\"RUA/AV\"] = df[\"RUA/AV\"].str.replace(\".\",\"\")\n",
    "        df[\"RUA/AV\"][df[\"RUA/AV\"].isin([\"rua\", \"rua doutor\", \"rua professor\"])] = \"rua\"\n",
    "        df[\"RUA/AV\"][df[\"RUA/AV\"].isin([\"al\", \"alameda\"])] = \"alameda\"\n",
    "        df[\"RUA/AV\"][df[\"RUA/AV\"].isin([\"av\", \"avenida\"])] = \"avenida\"\n",
    "        df[\"RUA/AV\"][df[\"RUA/AV\"].isin([\"trav\", \"travessa\"])] = \"travessa\"\n",
    "        df[\"RUA/AV\"][df[\"RUA/AV\"].isin([\"est\", \"estrada\"])] = \"estrada\"\n",
    "        df[\"RUA/AV\"] = df[\"RUA/AV\"].apply(lambda x: unidecode(x).strip().upper())\n",
    "        df.columns = cols_buy(cls = df.columns.tolist())\n",
    "        df[\"DATA_ANUNCIO\"] = date\n",
    "        df[\"DATA_RETIRADA\"] = \"-\"\n",
    "        return(df)\n",
    "\n",
    "df_cp_rs = get_data_in_shape(date = date, file_name = \"Dataset_AP_Comprar_Residencial\")\n",
    "df_al_rs = get_data_in_shape(date = date, file_name = \"Dataset_AP_Alugar_Residencial\")\n",
    "df_cp_cc = get_data_in_shape(date = date, file_name = \"Dataset_AP_Comprar_Comercial\")\n",
    "df_al_cc = get_data_in_shape(date = date, file_name = \"Dataset_AP_Alugar_Comercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(df, dataset):\n",
    "\n",
    "    try:\n",
    "\n",
    "        original = pd.read_csv(dataset + \".txt\")\n",
    "\n",
    "        dimension = original.shape[0]\n",
    "\n",
    "        new_keys = df[\"KEY\"].tolist()\n",
    "\n",
    "        original[~original.KEY.isin(new_keys)][\"DATA_RETIRADA\"] = date\n",
    "        original = pd.concat([original, df], axis = 0)\n",
    "\n",
    "        original.drop_duplicates(\n",
    "            subset=[\n",
    "                'TIPO', 'OFERTA', 'VALORES', 'VALORES_PROMO', 'RUA/AV', 'LOGRADOURO',\n",
    "                'LOCATION', 'CITY', 'STATE', 'AREA', 'QUARTO', 'GARAGEM', 'BANHEIRO',\n",
    "                'KEY', 'DATA_RETIRADA'\n",
    "            ], inplace = True\n",
    "        )\n",
    "\n",
    "        if original.shape[0] > dimension:\n",
    "            original.to_csv(dataset + \".txt\", index = False)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        df.to_csv(dataset + \".txt\", index = False)\n",
    "\n",
    "merge_dataset(df = df_cp_rs, dataset = \"Dataset_AP_Comprar_Residencial\")\n",
    "merge_dataset(df = df_al_rs, dataset = \"Dataset_AP_Alugar_Residencial\")\n",
    "merge_dataset(df = df_cp_cc, dataset = \"Dataset_AP_Comprar_Comercial\")\n",
    "merge_dataset(df = df_al_cc, dataset = \"Dataset_AP_Alugar_Comercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAIRRO</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>LAT_STREET</th>\n",
       "      <th>LON_STREET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BELA VISTA, PORTO ALEGRE</td>\n",
       "      <td>RUA JARAGUA, PORTO ALEGRE</td>\n",
       "      <td>-29.9693916</td>\n",
       "      <td>-51.6079471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIGIENOPOLIS, PORTO ALEGRE</td>\n",
       "      <td>RUA ARI MARINHO, PORTO ALEGRE</td>\n",
       "      <td>-30.0132564</td>\n",
       "      <td>-51.1796773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOINHOS DE VENTO, PORTO ALEGRE</td>\n",
       "      <td>RUA MARQUES DO HERVAL, PORTO ALEGRE</td>\n",
       "      <td>-30.0211299</td>\n",
       "      <td>-51.066669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JARDIM SABARA, PORTO ALEGRE</td>\n",
       "      <td>RUA JOAO ERNESTO SCHMIDT, PORTO ALEGRE</td>\n",
       "      <td>-30.0347512</td>\n",
       "      <td>-51.1468274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AZENHA, PORTO ALEGRE</td>\n",
       "      <td>AVENIDA FLORIANOPOLIS, PORTO ALEGRE</td>\n",
       "      <td>-30.0544875</td>\n",
       "      <td>-51.2133384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>CENTRO HISTORICO, PORTO ALEGRE</td>\n",
       "      <td>RUA DOS ANDRADAS, PORTO ALEGRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>PRAIA DE BELAS, PORTO ALEGRE</td>\n",
       "      <td>AVENIDA PADRE CACIQUE, PORTO ALEGRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>PRAIA DE BELAS, PORTO ALEGRE</td>\n",
       "      <td>AVENIDA PADRE CACIQUE, PORTO ALEGRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>AUXILIADORA, PORTO ALEGRE</td>\n",
       "      <td>RUA 24 DE OUTUBRO, PORTO ALEGRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>PASSO DA AREIA, PORTO ALEGRE</td>\n",
       "      <td>AVENIDA ASSIS BRASIL, PORTO ALEGRE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48912 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              BAIRRO                                 ADDRESS   \n",
       "0           BELA VISTA, PORTO ALEGRE               RUA JARAGUA, PORTO ALEGRE  \\\n",
       "1         HIGIENOPOLIS, PORTO ALEGRE           RUA ARI MARINHO, PORTO ALEGRE   \n",
       "2     MOINHOS DE VENTO, PORTO ALEGRE     RUA MARQUES DO HERVAL, PORTO ALEGRE   \n",
       "3        JARDIM SABARA, PORTO ALEGRE  RUA JOAO ERNESTO SCHMIDT, PORTO ALEGRE   \n",
       "4               AZENHA, PORTO ALEGRE     AVENIDA FLORIANOPOLIS, PORTO ALEGRE   \n",
       "...                              ...                                     ...   \n",
       "1901  CENTRO HISTORICO, PORTO ALEGRE          RUA DOS ANDRADAS, PORTO ALEGRE   \n",
       "1902    PRAIA DE BELAS, PORTO ALEGRE     AVENIDA PADRE CACIQUE, PORTO ALEGRE   \n",
       "1903    PRAIA DE BELAS, PORTO ALEGRE     AVENIDA PADRE CACIQUE, PORTO ALEGRE   \n",
       "1904       AUXILIADORA, PORTO ALEGRE         RUA 24 DE OUTUBRO, PORTO ALEGRE   \n",
       "1905    PASSO DA AREIA, PORTO ALEGRE      AVENIDA ASSIS BRASIL, PORTO ALEGRE   \n",
       "\n",
       "       LAT_STREET   LON_STREET  \n",
       "0     -29.9693916  -51.6079471  \n",
       "1     -30.0132564  -51.1796773  \n",
       "2     -30.0211299   -51.066669  \n",
       "3     -30.0347512  -51.1468274  \n",
       "4     -30.0544875  -51.2133384  \n",
       "...           ...          ...  \n",
       "1901          NaN          NaN  \n",
       "1902          NaN          NaN  \n",
       "1903          NaN          NaN  \n",
       "1904          NaN          NaN  \n",
       "1905          NaN          NaN  \n",
       "\n",
       "[48912 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas\n",
    "\n",
    "df1 = pandas.read_csv(\"Dataset_AP_Comprar_Residencial.txt\")\n",
    "df1[\"ADDRESS\"] = df1[\"RUA/AV\"] + \" \" + df1[\"LOGRADOURO\"] + \", \" + df1[\"CITY\"]\n",
    "df1[\"BAIRRO\"] = df1[\"LOCATION\"] + \", \" + df1[\"CITY\"]\n",
    "\n",
    "df2 = pandas.read_csv(\"Dataset_AP_Comprar_Comercial.txt\")\n",
    "df2[\"ADDRESS\"] = df2[\"RUA/AV\"] + \" \" + df2[\"LOGRADOURO\"] + \", \" + df2[\"CITY\"]\n",
    "df2[\"BAIRRO\"] = df2[\"LOCATION\"] + \", \" + df2[\"CITY\"]\n",
    "\n",
    "df3 = pandas.read_csv(\"Dataset_AP_Alugar_Residencial.txt\")\n",
    "df3[\"ADDRESS\"] = df3[\"RUA/AV\"] + \" \" + df3[\"LOGRADOURO\"] + \", \" + df3[\"CITY\"]\n",
    "df3[\"BAIRRO\"] = df3[\"LOCATION\"] + \", \" + df3[\"CITY\"]\n",
    "\n",
    "df4 = pandas.read_csv(\"Dataset_AP_Alugar_Comercial.txt\")\n",
    "df4[\"ADDRESS\"] = df4[\"RUA/AV\"] + \" \" + df4[\"LOGRADOURO\"] + \", \" + df4[\"CITY\"]\n",
    "df4[\"BAIRRO\"] = df4[\"LOCATION\"] + \", \" + df4[\"CITY\"]\n",
    "\n",
    "\n",
    "streets = pandas.concat([\n",
    "    df1[[\"BAIRRO\", \"ADDRESS\", \"LAT_STREET\", \"LON_STREET\"]],\n",
    "    df2[[\"BAIRRO\", \"ADDRESS\", \"LAT_STREET\", \"LON_STREET\"]],\n",
    "    df3[[\"BAIRRO\", \"ADDRESS\", \"LAT_STREET\", \"LON_STREET\"]],\n",
    "    df4[[\"BAIRRO\", \"ADDRESS\", \"LAT_STREET\", \"LON_STREET\"]]\n",
    "    ], axis = 0\n",
    ")\n",
    "\n",
    "processing_streets = pandas.DataFrame(streets[\"ADDRESS\"].unique().tolist())\n",
    "processing_streets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import os\n",
    "\n",
    "def street_lat_long():\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "    for f in os.listdir():\n",
    "        if f.startswith(\"Dataset_\"):\n",
    "            df = pd.read_csv(f)\n",
    "            if \"LAT_STREET\" not in df.columns:\n",
    "                df[\"LAT_STREET\"] = \"-\"\n",
    "                df[\"LON_STREET\"] = \"-\"\n",
    "            df[\"ADDRESS\"] = df[\"RUA/AV\"] + \" \" + df[\"LOGRADOURO\"] + \", \" + df[\"CITY\"]\n",
    "            list_address = df[\"ADDRESS\"].unique().tolist()\n",
    "            for add in list_address:\n",
    "                try:\n",
    "                    location = geolocator.geocode(add)\n",
    "                    df[\"LAT_STREET\"][(df.LAT_STREET == \"-\") & (df.ADDRESS == add)] = location.latitude\n",
    "                    df[\"LON_STREET\"][(df.LON_STREET == \"-\") & (df.ADDRESS == add)] = location.longitude\n",
    "                except:\n",
    "                    df[\"LAT_STREET\"][df.ADDRESS == add] = \"-\"\n",
    "                    df[\"LON_STREET\"][df.ADDRESS == add] = \"-\"\n",
    "            df.drop(\"ADDRESS\", axis = 1, inplace = True)\n",
    "            df.to_csv(f, index = False)\n",
    "\n",
    "street_lat_long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geobr\n",
    "\n",
    "geobr.read_neighborhood()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
